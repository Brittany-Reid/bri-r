@misc{RN18,
 abstract = {Online participant recruitment platforms such as Prolific have been gaining popularity in research, as they enable researchers to easily access large pools of participants. However, participant quality can be an issue; participants may give incorrect information to gain access to more studies, adding unwanted noise to results. This paper details our experience recruiting participants from Prolific for a user study requiring programming skills in Node.js, with the aim of helping other researchers conduct similar studies. We explore a method of recruiting programmer participants using prescreening validation, attention checks and a series of programming knowledge questions. We received 680 responses, and determined that 55 met the criteria to be invited to our user study. We ultimately conducted user study sessions via video calls with 10 participants. We conclude this paper with a series of recommendations for researchers. },
 author = {Reid, Brittany and Wagner, Markus and d'Amorim, Marcelo and Treude, Christoph},
 title = {Software engineering user study recruitment on prolific: An experience report},
 type = {Conference Paper},
 url = {https://arxiv.org/abs/2201.05348},
 year = {2022}
}
